<!DOCTYPE html>

<html>
<head>
  <meta content="text/html; charset=utf-8" http-equiv="content-type">
  <link href="favicon.ico" rel="shortcut icon">

  <title>Mobile Robots (EMOR) tutorials</title>
  <link href="style.css" rel="stylesheet">
  <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>
</head>

<body>
  <div class="page">

    <div class="heading_body">
      <div style="text-align:left; font-size:160%;">
        <strong>Mobile Robots (EMOR)</strong> tutorials.
      </div>
    </div>

    <div class="content_body">
      <div id="menu">
        <a class="top" href="index.html">Introduction</a>
        <a href="setup.html">Setup</a>
        <a href="example.html">Example project</a>
        <a href="project1.html">Project 1</a>
        <a href="project2.html">Project 2</a>
        <a href="project3.html">Project 3</a>
          <a class="submenu" href="project3.html#loc1">Localization without LIDAR</a>
          <a class="submenu" href="project3.html#loc2">Localization with LIDAR</a>
          <a class="submenu" href="project3.html#wavefront">Wavefront</a>

        <a href="api.html">API Documentation</a>
        <a href="tips.html">Tips & Tricks</a>
        <a href="troubleshooting.html">Troubleshooting</a>
      </div>

      <div class="content">

        <p>To be announced.</p>

        <a id="info" name="info"></a>
        <a id="description" name="description"></a>
        <h1 style="margin-top: 0">Description</h1>

          <p>The goal of the third EMOR tutorial is to introduce the localization and path planning.
          The localization is implemented with the Particle Filter and the path planning is presented on the Wavefront algorithm.</p>

        <a id="loc1" name="loc1"></a>
        <h1 style="margin-top: 0">Localization without LIDAR</h1>
          <p>The goal of this subtask is to write a program enabling youBot to localize itself using the particle filter.
          Otherwise than in the previous projects, it is assumed that the robot does not know its pose.
          Instead, the odometry is provided in every control step.
          Thus, the pose of the robot must estimated by using odometry data and a provided map.</p>

          <p>The map files are shipped together with the scene files <code>*.ttt</code>. Each scene file <code>&lt;filename&gt;.ttt</code> has its map file <code>&lt;filename&gt;.png</code>.
          For more information on reading and using the map files in Matlab please see the Programming Hints section.
          You can use all scenes provided in the vrep_env directory to experiment and test your solution.</p>

          <p>You can use the Particle Filter implementation from the Robotics Toolbox as a reference. Please see the file <code>~/ws_emor/emor_trs/matlab/rvctools/robot/ParticleFilter.m</code>.
          Note that the implementation in the Robotics Toolbox cannot be used directly in your program. You need to implement your own filter in the control callback function
          <code>solution3a</code>.</p>

          <p>The idea of the Particle Filter is to generate a big number of samples uniformly distributed on the map.
          Each sample is hypothesis on the pose of the robot.
          In each control step the Particle Filter executes three stages:
          <ol>
            <li>predict: new poses for all samples are predicted by using the odometry data (and some random noise is added),</li>
            <li>observe: all samples are evaluated using the real sensor data and the model of the sensor,</li>
            <li>select: the samples with the highest scores are duplicated, and the low-scored samples are discarded.</li>
          </ol>
          A sample is just a state vector of the robot, i.e. [x,y,fi].
          </p>

          <p>As the LIDAR data is discarded in this subtask, the only information about the robot state is its odometry.
          Additionally, if the robot is controlled using some reactive obstacle avoidance controller (e.g. moving along the walls),
          we can assume that the robot is not in a collision with the environment.
          That information can be used instead of the LIDAR data, to remove all particles that collide with the obstacles in the map.
          Such colliding particles are not likely if an obstacle avoidance algorithm is used to control the robot.</p>

          <p>The video below shows an example execution of the control program with the particle filter implementation:</p>
          <div>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/x4VNiMJ7qL8" frameborder="0" allowfullscreen></iframe>
          </div>

          <h2>Programming hints</h2>

            <p>The callback function for this task must be declared as:
<code class="block">function [forwBackVel, leftRightVel, rotVel, finish] = solution3a(pts, contacts, delta_position, delta_orientation, varargin)
% The control loop callback function - the solution for task 3A
...
end</code>
            where the odometry data: <code>delta_position</code> is the change of the position expressed in the local robot's frame (a 2-element vector) and <code>delta_orientation</code> is the change of the orientation (a scalar value).
            To run the simulation providing the odometry data instead the absolute pose data, run in the Matlab console:
<code class="block">run_simulation_odom(@solution3a, false)</code>
            </p>

            <p>You can use the "move along the walls" controller from the previous project to generate collision-less trajectory.</p>

            <p>Initialize the Particle Filter in the initial state and then update the filter in every control iteration (independently of the FSM).</p>

            <p>To read a map file use the <code>imread</code> Matlab function, e.g.:
<code class="block">map = imread('vrep_env/map2.png');</code>
            The map is a matrix, where each cell corresponds to the occupancy state of a particular place in the scene.
            The state of a cell is either 0 (occupied) or 255 (free), e.g. map(1,1) == 0 means that the scene section
            with the lowest x and the lowest y is occupied.
            The map is indexed in ranges:
            <ul>
              <li>for x: from 1 to size_x,</li>
              <li>for y: from 1 to size_y,</li>
            </ul>
            where <code>[size_y, size_x] = size(map);</code>.
            As the map corresponds to the scene, please take into account the units conversion, e.g.
            if the scene spans from -5 m to 5 m in x direction and from -10 m to 10 m in y direction, and the size of the map is 100 by 200 cells (<code>size(map)==[200,100]</code>),
            the units conversions are:
            <ul>
              <li><code>x_map=round( 100*((x_world-(-5))/(5-(-5))) )</code>,</li>
              <li><code>y_map=round( 200*((x_world-(-10))/(10-(-10))) )</code>.</li>
            </ul>
            The inverse conversion can be easily calculated.
            Please note that the indexing of the map is (y,x).</p>

            <p>All environments span from -7.5 m to 7.5 m in both x and y directions, and the size of all maps is 100 by 100.</p>

            <p>Use plots to visualise the state of particle filter.
            Please refer to the plotting and image drawing API for Matlab:
            <ul>
              <li><a href="http://www.mathworks.com/help/matlab/ref/plot.html" target="_blank">http://www.mathworks.com/help/matlab/ref/plot.html</a>,</li>
              <li><a href="http://www.mathworks.com/help/images/ref/imshow.html" target="_blank">http://www.mathworks.com/help/images/ref/imshow.html</a>.</li>
            </ul>
            Information on plotting an image in the background of a plot is avaiable in www page:
            <ul>
              <li><a href="http://www.peteryu.ca/tutorials/matlab/plot_over_image_background" target="_blank">http://www.peteryu.ca/tutorials/matlab/plot_over_image_background</a>.</li>
            </ul>

          <h2 style="margin-top: 0">Grading (5 points total)</h2>
            <ol>
              <li>Units conversion, proper transformations, plotting (2p)</li>
              <li>Detection of particle collisions with obstacles in the map (1p)</li>
              <li>Particle filter (2p)</li>
            </ol>

        <a id="loc2" name="loc2"></a>
        <h1 style="margin-top: 0">Localization with LIDAR</h1>
          <p>The goal of this subtask is to extend the previous subtask and to utilize the LIDAR data in the particle filter.
          The main idea is the same as in the previous subtask.
          However, the observe stage of the filter is not degenerated.
          You can use similar observe stage of the Particle Filter as it is implemented in the Robotics Toolbox.
          Please see the file <code>~/ws_emor/emor_trs/matlab/rvctools/robot/ParticleFilter.m</code>.</p>

          <p>You have to implement the control callback function <code>solution3b</code> in this subtask.
          To run the simulation with the odometry data, type in the Matlab console:
<code class="block">run_simulation_odom(@solution3b, false)</code>
          Also, you have to implement the sensor model, i.e. the function that generates the sensor readings for a sample and the map.
          It is recommended to use only a subset of the LIDAR data, as using all 684 laser rays could be too slow.
          You can use three laser rays, as in the video example presented below:
          <ul>
            <li>angle: <code>-pi/4</code>, index in the <code>pts</code> array: 214,</li>
            <li>angle: <code>0</code>, index in the <code>pts</code> array: 342,</li>
            <li>angle: <code>pi/4</code>, index in the <code>pts</code> array: 470.</li>
          </ul>
          However, you are free to experiment on your own!</p>
          <div>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/Xlt1aoz5fUw" frameborder="0" allowfullscreen></iframe>
          </div>

          <h2 style="margin-top: 0">Grading (5 points total)</h2>
            <ol>
              <li>Units conversion, proper transformations, plotting (1p)</li>
              <li>Sensor model (2p)</li>
              <li>Particle filter (2p)</li>
            </ol>

          <h2>Programming hints</h2>
            You can use the following function to predict LIDAR sensor readings for the particles:
<code class="block">function result = predSensors(map, x, angle, dim_x, dim_y)
    %Prediction of LIDAR sensor readings
    %Format:
    %               [x y]=predSensors(map,x,angle,dim_x, dim_y)
    %
    %Input:
    %               map:   the map as a matrix created by imread function
    %               x:     the array of state vectors, where each state
    %                      vector is [x, y, fi]
    %               angle: the angle of the LIDAR ray (in radians)
    %               dim_x: the size of the map in the x axis direction
    %               dim_y: the size of the map in the y axis direction
    %
    %Output:
    %               result: the vector of distances from each state
    %               particle to the closest obstacle in the direction of
    %               particle's orientation and ray angle
    %
    %Usage example:
    %               pfnparticles = 10000;
    %               pfx = (2*pfrandstream.rand([pfnparticles,3]) - 1) * diag([7.5, 7.5, pi]);
    %               dist=predSensors(map, pfx, pi/4, [-7.5,7.5], [-7.5, 7.5]);
    
    n = [cos(x(:,3)-pi/2+angle), sin(x(:,3)-pi/2+angle)];

    [x_count, ~] = size(x);
    [map_y_size, map_x_size] = size(map);
    ix = round( map_x_size*((x(:,1)-dim_x(1)) / (dim_x(2)- dim_x(1))) );
    iy = round( map_y_size*((x(:,2)-dim_y(1)) / (dim_y(2)- dim_y(1))) );

    result = zeros( [x_count, 1] ) - 1;
    
    for idx = 1:x_count
        if ix(idx) < 1 || iy(idx) < 1 || iy(idx) > map_y_size || ix(idx) > map_x_size,
            result(idx) = 100;
        elseif map(iy(idx), ix(idx)) == 0,
            result(idx) = 100;
        end
    end

    px_size = (dim_x(2) - dim_x(1)) / map_x_size;
    steps = 5 / px_size;
    for i = 1:steps
        d = (px_size * i) * n;
        ix = round(map_x_size*(x(:,1)+d(:,1)-dim_x(1)) / (dim_x(2)- dim_x(1)));
        iy = round(map_y_size*(x(:,2)+d(:,2)-dim_y(1)) / (dim_y(2)- dim_y(1)));
        for idx = 1:x_count
            if result(idx) > 0
                continue
            end
            if ix(idx) < 1 || iy(idx) < 1 || iy(idx) > map_y_size || ix(idx) > map_x_size,
                result(idx) = px_size * i;
            elseif map(iy(idx), ix(idx)) == 0,
                result(idx) = px_size * i;
            end
        end
    end

    for idx = 1:x_count
        if result(idx) < 0
            result(idx) = 5;
        end
    end
end</code>

        <a id="wavefront" name="wavefront"></a>
        <h1>Path planning with Wavefront</h2>
          <p>This algorithm is responsible for finding the optimal route to the goal in the planning phase, i.e. before leaving the initial pose.
          For this purpose it analyses the map and generates the path and in an off-line manner.
          When finished, the robot simply follows the generated path.
          The generation of the path works as follows:
          <ol>
            <li> Set the cost of target position (cell) to 0 </li>
            <li> For a given cell (neighbouring with cells with assigned cost) find the neighbour with lowest cost lc, set cost of the currently analyzed cell to lc+1 </li>
            <li> Repeat step 2 for each cell until all cells (including the initial one) have assigned costs </li>
            <li> Find the path starting from the initial cell to the destination, by selecting the neighbouring cell with the smallest cost</li>
          </ol>
          More detailed description of the Wavefront can be found in the EMOR lectures, as well as on <a href="http://www.societyofrobots.com/programming_wavefront.shtml">this website</a>.</p>

          <p>In this subtask, it is assumed that the pose of the robot is known, so to run the simulation (with absolute pose provided) type in the Matlab console:
<code class="block">run_simulation(@solution3c, false, [goal_x, goal_y])</code>
          where <code>solution3c</code> is the control callback function you have to implement, and <code>[goal_x, goal_y]</code> is the position of the goal point.</p>

          <p>It is recommended to plot the state of the Wavefront planner for debugging and visualisation purposes.</p>

          <p>The planner should run in the initial state. The generated collision-less trajectory should be executed using the controller implemented in the
          Project 1 (Following a trajectory to the destination point). The robot should move to the subsequent cells on the path generated by Wavefront.</p>

          <h2>Requirements</h2>
            The robot is supposed to move in diverse environments.

          <h2 style="margin-top: 0">Grading (5 points total)</h2>
            <ol>
              <li>Units conversion, proper transformations, plotting (2p)</li>
              <li>Wavefront implementation (2p)</li>
              <li>Trajectory execution (1p)</li>
            </ol>
    </div>

      <div class="trailer_body">
        <div style="width: 100%; display: table;">
          <div style="display: table-row">
            <div style="width: 50%; display: table-cell; vertical-align: bottom;">
              Valid <a href="http://validator.w3.org/check?uri=referer">HTML5</a> and <a href="http://jigsaw.w3.org/css-validator/check/referer">CSS</a>
            </div>

            <div style="width: 50%; display: table-cell; text-align: right; vertical-align: bottom;">
              &copy; Renaud Detry 2014<br>
              &copy; Dawid Seredyński, Tomasz Kornuta 2016<br>
              This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</body>
</html>
